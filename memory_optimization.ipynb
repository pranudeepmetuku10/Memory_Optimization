{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75363f74-8f04-4b59-a9fe-9192c08e9938",
   "metadata": {},
   "source": [
    "## Step 1 — Load Data (Default Settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "854451e1-4681-43eb-9a3a-49dc91648f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (10000, 10)\n",
      "\n",
      "Data types:\n",
      "customer_id             int64\n",
      "age                     int64\n",
      "region                 object\n",
      "customer_type          object\n",
      "total_purchases         int64\n",
      "total_spent           float64\n",
      "avg_purchase_value    float64\n",
      "satisfaction_score    float64\n",
      "account_status         object\n",
      "referral_source        object\n",
      "dtype: object\n",
      "\n",
      "Memory usage (before optimization): 2.59 MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load with default settings\n",
    "df = pd.read_csv('customer_data.csv')\n",
    "\n",
    "# Check the shape\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"\\nData types:\\n{df.dtypes}\")\n",
    "\n",
    "# Measure memory usage\n",
    "memory_before = df.memory_usage(deep=True).sum() / 1024**2  # Convert to MB\n",
    "print(f\"\\nMemory usage (before optimization): {memory_before:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1007b872-ae28-4bfd-b184-7a9bcee38206",
   "metadata": {},
   "source": [
    "\n",
    "## Step 2 — Optimize Memory Usage\n",
    "\n",
    "```\n",
    "(a) Select only required columns\n",
    "\n",
    "customer_id, age, region, customer_type, total_spent, satisfaction_score\n",
    "\n",
    "(b) Apply optimal dtypes\n",
    "\t•\tcustomer_id → int32\n",
    "\t•\tage → int8\n",
    "\t•\tregion → category\n",
    "\t•\tcustomer_type → category\n",
    "\t•\ttotal_spent → float32\n",
    "\t•\tsatisfaction_score → float32\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc90e9ed-a223-4bc3-8d3c-05fe3c7fecae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Memory usage (after optimization): 0.1440 MB\n",
      "\n",
      "Memory reduction: 94.45%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Select only needed columns\n",
    "use_cols = [\n",
    "    \"customer_id\", \"age\", \"region\", \n",
    "    \"customer_type\", \"total_spent\", \"satisfaction_score\"\n",
    "]\n",
    "\n",
    "# Define optimal dtypes\n",
    "opt_dtypes = {\n",
    "    \"customer_id\": \"int32\",\n",
    "    \"age\": \"int8\",\n",
    "    \"region\": \"category\",\n",
    "    \"customer_type\": \"category\",\n",
    "    \"total_spent\": \"float32\",\n",
    "    \"satisfaction_score\": \"float32\"\n",
    "}\n",
    "\n",
    "# Load with optimizations\n",
    "df_optimized = pd.read_csv(\n",
    "    \"customer_data.csv\",\n",
    "    usecols=use_cols,\n",
    "    dtype=opt_dtypes\n",
    ")\n",
    "\n",
    "# Measure optimized memory\n",
    "memory_after = df_optimized.memory_usage(deep=True).sum() / 1024**2\n",
    "print(f\"\\nMemory usage (after optimization): {memory_after:.4f} MB\")\n",
    "\n",
    "# Calculate improvement\n",
    "reduction_percent = ((memory_before - memory_after) / memory_before) * 100\n",
    "print(f\"\\nMemory reduction: {reduction_percent:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90f4d1c-8478-440e-bb53-64f2db4b6876",
   "metadata": {},
   "source": [
    "## Step 3 — Verify Optimized DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0978bf32-2323-4e5c-965b-092b240e1a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimized shape: (10000, 6)\n",
      "\n",
      "Optimized dtypes:\n",
      " customer_id              int32\n",
      "age                       int8\n",
      "region                category\n",
      "customer_type         category\n",
      "total_spent            float32\n",
      "satisfaction_score     float32\n",
      "dtype: object\n",
      "\n",
      "First few rows:\n",
      "    customer_id  age     region customer_type  total_spent  satisfaction_score\n",
      "0            1   56  Northeast          Gold   246.130005                 1.1\n",
      "1            2   69  Northeast        Silver  7928.109863                 3.5\n",
      "2            3   46    Midwest        Bronze    20.570000                 3.8\n",
      "3            4   32  Southeast        Bronze  3439.129883                 2.6\n",
      "4            5   60       West      Platinum  4945.830078                 1.7\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nOptimized shape:\", df_optimized.shape)\n",
    "print(\"\\nOptimized dtypes:\\n\", df_optimized.dtypes)\n",
    "print(\"\\nFirst few rows:\\n\", df_optimized.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a7429f-5e67-4585-af67-bfbed11e22ed",
   "metadata": {},
   "source": [
    "## Final Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc84ba7-b76b-4f31-beca-c4e6b9e93bb4",
   "metadata": {},
   "source": [
    "| Metric                               | Your Result |\n",
    "|--------------------------------------|-------------|\n",
    "| Memory usage before optimization     | 2.59 MB     |\n",
    "| Memory usage after optimization      | 0.1440 MB   |\n",
    "| Memory reduction percentage          | 94.45%      |\n",
    "| Number of columns (before)           | 10          |\n",
    "| Number of columns (after)            | 6           |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ca28b4-08fc-4f76-bc70-6ca8a9bcccfe",
   "metadata": {},
   "source": [
    "## Reflection Questions\n",
    "\n",
    "### 1. Which optimization technique had the biggest impact on memory usage? Why?\n",
    "\n",
    "The biggest impact came from dtype optimization, especially converting object columns like region and customer_type into category, and downcasting numeric columns from int64/float64 to smaller types like int32, int8, and float32. Category encoding saves a lot of memory because repeated strings are internally stored only once. Numeric downcasting cuts column sizes in half or more, which is especially impactful in large datasets.\n",
    "\n",
    "### 2. If the dataset were 100× larger, how would these optimizations help your workflow?\n",
    "\n",
    "For a dataset 100× larger, these optimizations would drastically reduce RAM usage, making it possible to load and analyze the data without crashes or slowdowns. The smaller memory footprint speeds up file loading, filtering, grouping, and model training. It also enables more efficient data pipelines and reduces cloud compute costs when working with very large datasets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
